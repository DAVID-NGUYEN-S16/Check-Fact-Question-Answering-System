{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to C:\\Users\\nguye\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7419e12f236445899c2e90976a52339c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f237dec0f8c4ae3a104840914aa23b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at nguyenvulebinh/vi-mrc-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForQuestionAnswering, default_data_collator, get_scheduler\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "# from transformers.models.bartpho.tokenization_bartpho_fast import BartphoTokenizerFast\n",
    "from transformers import AutoModelForQuestionAnswering, default_data_collator, get_scheduler\n",
    "from torch import nn\n",
    "# import evaluate\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from transformers.optimization import SchedulerType\n",
    "import re\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from transformers import AutoTokenizer, RobertaModel\n",
    "from transformers import get_linear_schedule_with_warmup, AutoTokenizer, AutoModel, logging\n",
    "from huggingface_hub import login\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from model.models import ClaimVerification\n",
    "from model.models import ModelQA\n",
    "from util.process_data import split_sentence, preprocess_text, process_data\n",
    "from util.function import get_top_context\n",
    "from util.function import check_evidence, classify_nei\n",
    "login(token='xxxx.xxx')\n",
    "device = \"cpu\"\n",
    "tokenizer_rs = AutoTokenizer.from_pretrained(\"MoritzLaurer/ernie-m-large-mnli-xnli\")\n",
    "tokenizer_f1 = AutoTokenizer.from_pretrained(\"nguyenvulebinh/vi-mrc-base\")\n",
    "tokenizer_evidence = AutoTokenizer.from_pretrained(\"MoritzLaurer/ernie-m-large-mnli-xnli\")\n",
    "tokenizer_3_class = AutoTokenizer.from_pretrained(\"MoritzLaurer/ernie-m-large-mnli-xnli\")\n",
    "\n",
    "checkpoint_classify_3_class = torch.load(f\"./weight_model/classify-3-class/best_acc.pth\",  map_location=torch.device('cpu'))\n",
    "model_classify_3_class = ClaimVerification(n_classes=3, name_model = \"MoritzLaurer/ernie-m-large-mnli-xnli\")\n",
    "model_classify_3_class.load_state_dict(checkpoint_classify_3_class)\n",
    "\n",
    "checkpoint_evidence = torch.load(f\"./weight_model/evidence-by-classify/best_acc.pth\", map_location=torch.device('cpu'))\n",
    "model_evidence= ClaimVerification(n_classes=2, name_model = \"MoritzLaurer/ernie-m-large-mnli-xnli\")\n",
    "model_evidence.load_state_dict(checkpoint_evidence)\n",
    "\n",
    "checkpoint_evidence_f1 = torch.load(f\"./weight_model/weght-model-base/best_model.pth\",  map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "best_model_state_dict_evidence_f1 = checkpoint_evidence_f1['model_state_dict']\n",
    "model_evidence_f1 = ModelQA(name_model = \"nguyenvulebinh/vi-mrc-base\")\n",
    "model_evidence_f1.load_state_dict(best_model_state_dict_evidence_f1)\n",
    "\n",
    "checkpoint_classify_rs = torch.load(f\"./weight_model/classify-2-class/model_rs/best_acc.pth\",  map_location=torch.device('cpu'))\n",
    "model_classify_rs = ClaimVerification(n_classes=2, name_model = \"MoritzLaurer/ernie-m-large-mnli-xnli\")\n",
    "model_classify_rs.load_state_dict(checkpoint_classify_rs)\n",
    "\n",
    "\n",
    "del checkpoint_evidence\n",
    "del checkpoint_classify_rs\n",
    "del checkpoint_classify_3_class\n",
    "del checkpoint_evidence_f1\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cag = ['NEI', 'SUPPORTED', 'REFUTED']\n",
    "\n",
    "def infer(sample):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    context = process_data(sample['context'])\n",
    "    claim = process_data(sample['claim'])\n",
    "    submit = {}\n",
    "\n",
    "\n",
    "    \n",
    "    lines = get_top_context(context = context, claim = claim, topk=5)\n",
    "    cnt = {0: 0, 1:0}\n",
    "    evidence = \"\"\n",
    "\n",
    "    \n",
    "    for line in lines:\n",
    "        encoding = tokenizer_evidence.encode_plus(\n",
    "            claim,\n",
    "            line,\n",
    "            truncation=\"only_second\",\n",
    "            add_special_tokens=True,\n",
    "            max_length=256,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        inputs = {\n",
    "                    'input_ids': encoding['input_ids'].reshape((1, 256)),\n",
    "                    'attention_masks': encoding['attention_mask'].reshape((1, 256)),\n",
    "                }\n",
    "\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_masks'].to(device)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_evidence(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "\n",
    "            )\n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "        cnt[pred[0].item()]+=1\n",
    "        \n",
    "        if pred[0].item() == 1:\n",
    "            evidence = line\n",
    "    if cnt[1] == 1:\n",
    "        submit = {\n",
    "                            'verdict': '1',\n",
    "                            'evidence': evidence\n",
    "        }\n",
    "    else:\n",
    "        \n",
    "        not_nei, evidence = check_evidence(context = context, claim = claim)\n",
    "\n",
    "        submit = {\n",
    "                            'verdict': '3',\n",
    "                            'evidence': evidence\n",
    "        }\n",
    "    \n",
    "    ##### Classify #############\n",
    "    \n",
    "    model_classify_3_class.eval()\n",
    "\n",
    "    context_sub = submit['evidence']\n",
    "    claim_sub = claim\n",
    "    encoding = tokenizer_3_class(\n",
    "            claim_sub,\n",
    "            context_sub,\n",
    "            truncation=\"only_second\",\n",
    "            add_special_tokens=True,\n",
    "            max_length=256,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "    inputs = {\n",
    "                'input_ids': encoding['input_ids'].reshape((1, 256)),\n",
    "                'attention_masks': encoding['attention_mask'].reshape((1, 256)),\n",
    "            }\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_masks'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_classify_3_class(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "\n",
    "        )\n",
    "    outputs = F.softmax(outputs, dim=1)\n",
    "\n",
    "    prob3class, pred = torch.max(outputs, dim=1)\n",
    "    \n",
    "    if pred.item() == 0:\n",
    "        submit['verdict'] ='NEI'\n",
    "        submit['evidence'] =''\n",
    "    else:\n",
    "        prob2class, output_rs = classify_nei(claim = claim, evidence = submit['evidence'], model = model_classify_rs, tokenazation = tokenizer_rs)\n",
    "        label_3class = cag[pred.item()]\n",
    "        label_2class = \"\"\n",
    "        if output_rs == 0:\n",
    "            label_2class = 'SUPPORTED'\n",
    "        else: \n",
    "            label_2class = 'REFUTED'\n",
    "            \n",
    "        submit['verdict'] = label_2class\n",
    "        \n",
    "        if label_3class != label_2class:\n",
    "    \n",
    "            if prob2class > prob3class:\n",
    "                submit['verdict'] = label_2class\n",
    "            else: submit['verdict'] = label_3class\n",
    "        \n",
    "    print(f\"Time infer: {time.time() - start_time}\")\n",
    "    submit['time'] = time.time() - start_time\n",
    "    return submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.models import ClaimVerification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
